---
title: "vLLM AI"
date: "2025-11-24"
description:
categories:
pageintoc: "200"
tags:
weight: "25"
---

<a id="vllm-intro"></a>

<!--# vLLM -->

## Appliance Description

[vLLM](https://docs.vllm.ai/) is a high-performance inference engine optimized for serving transformer LLMs with low latency, high throughput, token-level streaming, and efficient GPU memory usage. This appliance packages vLLM into a configurable OpenNebula VM image which is ready to run, with the purpose of simplified deployment on your OpenNebula cloud for inference workloads.

## Main Features

The vLLM appliance includes the following components for running LLMs:

- vLLM, the open source, high-perfomance engine for serving large language models with low latency and high throughput
- [Hugging Face - Transformers](https://huggingface.co/docs/transformers/index), one of the most widely adopted frameworks for deploying large language models
- Configurable deployment options and behavior, controlled by contextualization parameters

## Main References

- [vLLM](https://github.com/OpenNebula/one-apps/tree/master/appliances/Vllm) in the [OpenNebula one-apps](https://github.com/OpenNebula/one-apps) project
- [Full documentation](https://github.com/OpenNebula/one-apps/wiki/vllm_intro) for the vLLM appliance
- Download the vLLM appliance from the [OpenNebula Marketplace](https://marketplace.opennebula.io/appliance/d99e3dce-ad4c-45b9-8b22-eb1b3ade3ce9)
